#
# Copyright 2015 Formation Data Systems, Inc.
#
# TEST RESOURCES and TOPOLOGY
[user]
user_name       = root
password        = passwd

# The 'node' section defines a nodes parameters. The section must be prefixed
# with 'node' but is also used as a unique ID for the node.
#
[node1]
# Denotes this node will run an OM. Therefore, it is enabled automatically.
om              = true
# We'll start Redis with this node. Since all the other nodes listed
# here are on the same machine, we don't need to specify Redis for
# boot up on them.
redis           = true
# start influxdb
influxdb        = true
# IP of the node
ip              = localhost
# Fds root directory to use
fds_root        = /fds/node1
# Base port for that node, not needed if we don't run all nodes one physical machine
fds_port        = 7000
# By default we get all services on a node. Otherwise we' specify a 'services' option.

[node2]
enable          = true
ip              = localhost
fds_root        = /fds/node2
fds_port        = 7100
services        = dm,sm

[node3]
enable          = true
ip              = localhost
fds_root        = /fds/node3
fds_port        = 7200
services        = dm,sm

[node4]
enable          = true
ip              = localhost
fds_root        = /fds/node4
fds_port        = 7300
services        = dm,sm


# The 'policy' section defines a volume policy
#
[policy1]
# iops_min of the policy
iops_min = 100
# iops_max of the policy
iops_max = 500
# priority of the policy
priority = 1

# The 'volume' section defines a volume
[volume1]
# Name of the server AM node to attach to
client = node1
# The ID of the volume. Apparently ignored by the FDS system which assigns what it wants.
id     = 1
# The size of the volume
size   = 10000
# The volume access type, currently either 'object' or 'block'
access = object

[scenario_0_1]
log_marker       = Domain install
script           = [domain]
action           = install

[scenario_0_1_3]
log_marker      = Bring up domain
script          = [domain]
action          = boot-activate

[scenario_0_2]
# Verify nodes node1-4 are up
log_marker      = VERIFY ALL NODES CURRENTLY UP AND ACTIVE
script          = [verify]
state           = up
# Comma separated list of nodes.
fds_nodes       = node1,node2,nod3,node4

[scenario_0_3]
# Create a volume policy.
log_marker      = Create volume policy
script          = [policy1]
action          = create
delay_wait      = 10

[scenario_0_4]
# Create volume volume1
log_marker      = Create volume
script          = [volume1]
action          = create
delay_wait      = 10

[scenario_0_5]
# Attach volume volume1
log_marker      = Attach volume
script          = [volume1]
action          = attach

[scenario_0_6]
# Get an S3 authorization token
log_marker      = Get S3 auth token
script          = [testcases.TestOMIntFace.TestGetAuthToken]

[scenario_0_7]
# Get an S3 connection
log_marker      = Get S3 connection
script          = [testcases.TestS3IntFace.TestS3GetConn]

[scenario_0_8]
# Load some data into volume1 "bucket" using the S3 interface
log_marker      = Test S3 interface by putting blob
script          = [testcases.TestS3IntFace.TestS3LoadMBLOB]
# Command separated list of parameter names and values
param_names     = bucket
params          = volume1

[scenario_1]
# Run some IO
log_marker      = Fork/run longer IO workload
script          = [fork]
real-script     = [testcases.TrafficGen.RunTrafficGen]
param_names     = hostname,runtime,test_type,username
params          = localhost,120,PUT,admin

[scenario_1_1]
log_marker      = Set disk failure fault point in SM
script          = [testcases.TestFDSServiceMgt.TestServiceInjectFault]
param_names     = node,faultName
params          = node1,sm.objectstore.fail.metadata.disk

# Verify non-killed "remote" services are still running
[scenario_1_2]
script          = [verify]
state           = up
fds_nodes       = node1,node2,node3,node4
delay_wait      = 60

[scenario_2]
# Run some IO
log_marker      = Fork/run longer IO workload
script          = [fork]
real-script     = [testcases.TrafficGen.RunTrafficGen]
param_names     = hostname,runtime,test_type,username
params          = localhost,120,PUT,admin

[scenario_2_1]
log_marker      = Set disk failure fault point in SM
script          = [testcases.TestFDSServiceMgt.TestServiceInjectFault]
param_names     = node,faultName
params          = node2,sm.objectstore.fail.data.disk

# Verify non-killed "remote" services are still running
[scenario_2_2]
script          = [verify]
state           = up
fds_nodes       = node1,node2,node3,node4
delay_wait      = 60

[scenario_3]
# Run some IO
log_marker      = Fork/run longer IO workload
script          = [fork]
real-script     = [testcases.TrafficGen.RunTrafficGen]
param_names     = hostname,runtime,test_type,username
params          = localhost,120,PUT,admin

[scenario_3_1]
#log_marker      = Set disk failure fault point in SM
cript          = [testcases.TestFDSServiceMgt.TestServiceInjectFault]
param_names     = node,faultName
params          = node3,sm.objectstore.fail.data.disk

# Verify non-killed "remote" services are still running
[scenario_3_2]
#log_marker      = Verify remote services are still running 
script          = [verify]
state           = up
fds_nodes       = node1,node2,node3,node4
delay_wait      = 10

[scenario_4]
# Run some IO
log_marker      = Fork/run longer IO workload
script          = [fork]
real-script     = [testcases.TrafficGen.RunTrafficGen]
param_names     = hostname,runtime,test_type,username
#params          = localhost,120,PUT,admin

[scenario_4_1]
log_marker      = Set disk failure fault point in SM
script          = [testcases.TestFDSServiceMgt.TestServiceInjectFault]
param_names     = node,faultName
params          = node4,sm.objectstore.fail.metadata.disk

[scenario_5]
# Run some IO
log_marker      = Fork/run longer IO workload
script          = [fork]
real-script     = [testcases.TrafficGen.RunTrafficGen]
param_names     = hostname,runtime,test_type,username
params          = localhost,120,PUT,admin

[scenario_5_1]
log_marker      = Set disk failure fault point in SM
script          = [testcases.TestFDSServiceMgt.TestServiceInjectFault]
param_names     = node,faultName
params          = node1,sm.objectstore.fail.data.disk

[scenario_5_2]
# Run some IO
log_marker      = Fork/run longer IO workload
script          = [fork]
real-script     = [testcases.TrafficGen.RunTrafficGen]
param_names     = hostname,runtime,test_type,username
params          = localhost,120,PUT,admin

[scenario_5_3]
log_marker      = Verify remote services are still running 
script          = [verify]
state           = up
fds_nodes       = node1,node2,node3,node4
delay_wait      = 60

# Run the SM Checkers to confirm migration.
[scenario_6]
# Run SM checker and compare results.
log_marker      = Check SM Migration from all SMs.
# Try this one even if previous scenarios have failed just to see what happens.
script          = [testcases.SmChk.TestVerifyMigrations]

# Kill nodes
[scenario_7]
log_marker      = kill-uninstall domain
script          = [domain]
action          = kill-uninst
delay_wait      = 120

[scenario_8]
log_marker      = Verify all nodes and services are removed 
script          = [verify]
state           = down
fds_nodes       = node1,node2,node3,node4
