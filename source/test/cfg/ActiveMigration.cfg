#
# Copyright 2015 Formation Data Systems, Inc.
#
# Defines resource, configuration and steps to test
# Active Migration

[user]
user_name       = root
password        = passwd

# The 'node' section defines a nodes parameters. The section must be prefixed
# with 'node' but is also used as a unique ID for the node.
#
[node1]
# Denotes this node will run an OM. Therefore, it is enabled automatically.
om              = true
# We'll start Redis with this node. Since all the other nodes listed
# here are on the same machine, we don't need to specify Redis for
# boot up on them.
redis           = true
# start influxdb
influxdb        = true
# IP of the node
ip              = localhost
# Fds root directory to use
fds_root        = /fds/node1
# Base port for that node, not needed if we don't run all nodes one physical machine
fds_port        = 7000
# By default we get all services on a node. Otherwise we' specify a 'services' option.

[node2]
enable          = true
ip              = localhost
fds_root        = /fds/node2
fds_port        = 7100
services        = dm,sm

[node3]
enable          = true
ip              = localhost
fds_root        = /fds/node3
fds_port        = 7200
services        = dm,sm

[node4]
enable          = true
ip              = localhost
fds_root        = /fds/node4
fds_port        = 7300
services        = dm,sm

[node5]
enable          = true
ip              = localhost
fds_root        = /fds/node5
fds_port        = 7400
services        = sm

# The 'volume' section defines a volume
[volume1]
# Name of the server AM node to attach to
client = node1
# Apparently meant to be a Tenant ID.
id     = 1
# The size of the volume
size   = 10000
# The volume access type, currently either 'object' or 'block'
access = object

# The 'volume' section defines a volume
[volume2]
# Name of the server AM node to attach to
client = node1
# Apparently meant to be a Tenant ID.
id     = 2
# The size of the volume
size   = 10000
# The volume access type, currently either 'object' or 'block'
access = object

# The 'volume' section defines a volume
[volume3]
# Name of the server AM node to attach to
client = node1
# Apparently meant to be a Tenant ID.
id     = 3
# The size of the volume
size   = 10000000
# The volume access type, currently either 'object' or 'block'
access = block

# TEST STEPS or CASES or SCENARIOS
# Will be ordered according to scenario name.

[scenario_kill_uninst]
log_marker      = First clean up installation areas.
script          = [domain]
action          = kill-uninst

[scenario_0_1]
log_marker      = Install
script          = [node1]
action          = install

# Because we want to test migration, let's make the primaries all 4 so all the volumes and data
# are populated across all 4 nodes to ensure migration works.
[scenario_disable_restart_failed_children_processes_node1]
log_marker      = Change platform.conf to not restart killed processes automatically
script          = [testcases.TestFDSEnvMgt.TestModifyPlatformConf]
param_names     = current_string,replace_string,node
params          = restart_failed_children_processes.*, restart_failed_children_processes = false,node1

[scenario_disable_restart_failed_children_processes_nodeall]
log_marker      = Change platform.conf to not restart killed processes automatically
script          = [testcases.TestFDSEnvMgt.TestModifyPlatformConf]
param_names     = current_string,replace_string,node,applyAll
params          = restart_failed_children_processes.*, restart_failed_children_processes = false,node1,1

# Commenting out to keep enable_feature (which is setting both SM and DM migration feature)
# as default: on for SM migration and off for DM migration. We should not enable DM migration feature
# in master yet.
[scenario_change_platform]
log_marker      = Change global platform.conf
script          = [testcases.TestFDSEnvMgt.TestModifyPlatformConf]
param_names     = current_string,replace_string,node,applyAll
params          = enable_feature.*,enable_feature = true,node1,1

[scenario_change_platform1]
log_marker      = Change platform.conf
script          = [testcases.TestFDSEnvMgt.TestModifyPlatformConf]
param_names     = current_string,replace_string,node
params          = enable_feature.*,enable_feature = true,node1

[scenario_kill_node1]
log_marker      = Install
script          = [node1]
action          = kill

[scenario_0_3]
log_marker      = Start a single node domain by bringing up node node1
script          = [node1]
action          = boot-activate 

[scenario_0_4]
log_marker      = Verify node node1 is up
script          = [verify]
state           = up
# Comma separated list of nodes.
fds_nodes       = node1

[scenario_2_1]
log_marker      = Create volume voume1
script          = [volume1]
action          = create
delay_wait      = 10

[scenario_2_2]
log_marker      = Attach volume volume1
script          = [volume1]
action          = attach

[scenario_2_3]
log_marker      = Create volume volume2
script          = [volume2]
action          = create
delay_wait      = 10

[scenario_2_4]
log_marker      = Attach volume volume2
script          = [volume2]
action          = attach

[scenario_2_5]
log_marker      = Create volume volume3
script          = [volume3]
action          = create
delay_wait      = 10

[scenario_2_6]
log_marker      = Attach volume volume3
script          = [volume3]
action          = attach

[scenario_3_1]
log_marker      = Get an S3 authorization token
script          = [testcases.TestOMIntFace.TestGetAuthToken]

[scenario_3_2]
log_marker      = Get an S3 connection
script          = [testcases.TestS3IntFace.TestS3GetConn]

[scenario_3_3]
log_marker      = Load some data into volume1 "bucket" using the S3 interface
script          = [testcases.TestS3IntFace.TestS3LoadMBLOB]
# Command separated list of parameter names and values
param_names     = bucket
params          = volume1

[scenario_3_4]
log_marker      = Load some data into volume2 "bucket" using the S3 interface
script          = [testcases.TestS3IntFace.TestS3LoadLBLOB]
# Command separated list of parameter names and values
param_names     = bucket
params          = volume2

[scenario_4_1]
log_marker      = Boot and activate a second node.
script          = [node2]
action          = install-boot-activate

[scenario_4_2]
log_marker      = Wait for node 2 activation completion
script          = [waitforlog]
fds_node        = node2
service         = sm
logentry        = :NotifyDLTCloseCb
occurrences     = 1
maxwait         = 240

[scenario_4_3]
log_marker      = Verify nodes node1 and 2 are up
script          = [verify]
state           = up
# Comma separated list of nodes.
fds_nodes       = node1,node2

[scenario_wait_for_migration_msgs_node1]
log_marker      = Wait for migration client messages
script          = [waitforlog]
fds_node        = node1
service         = dm
logentry        = Creating migration client
occurrences     = 4
maxwait         = 120

[scenario_wait_for_migration_msgs_node2]
log_marker      = Wait for migration client messages
script          = [waitforlog]
fds_node        = node2
service         = dm
logentry        = Migration executor received
occurrences     = 4
maxwait         = 120

[scenario_wait_for_processdeltablobs_node2]
log_marker      = Wait for at least one processingdeltablobs msg
script          = [waitforlog]
fds_node        = node2
service         = dm
logentry        = Processing incoming CtrlNotifyDeltaBlobsMsg
occurrences     = 1
atleastone      = 1
maxwait         = 120

# This should technically be just once, but the chances of this new node owning
# both volumes in its DMT is high, so set it to exactly 2 occurrences here
# to make sure that there's no regression in the migration code.
# BUT it's possible that it owns 0 or 1 or both volumes.
[scenario_wait_for_blobs_applied_node2]
log_marker      = Check to see exactly 2 Volumes should have blobs applied
script          = [waitforlog]
fds_node        = node2
service         = dm
logentry        = received non-empty blob message
occurrences     = 2
maxwait         = 120

[scenario_wait_for_IO_resume_node2]
log_marker      = Looking for VV to be unblocked.
script          = [waitforlog]
fds_node        = node2
service         = dm
logentry        = Applying forwards is complete
occurrences     = 4
maxwait         = 120

[scenario_4_4_start_writes]
log_marker      = Run some IO while migration should be happening
script          = [fork]
real-script     = [testcases.TrafficGen.TestTrafficGen]
param_names     = hostname,runtime,test_type,username,volume_name,n_conns,outstanding_reqs,timeout
params          = localhost,250,PUT,admin,volume1,1,1,5000

[scenario_4_5_start_writes]
log_marker      = Run some IO while migration should be happening
script          = [fork]
real-script     = [testcases.TrafficGen.TestTrafficGen]
param_names     = hostname,runtime,test_type,username,volume_name,n_conns,outstanding_reqs,timeout
params          = localhost,250,PUT,admin,volume3,1,1,5000

[scenario_5_1]
log_marker      = Boot and activate a third node.
script          = [node3]
action          = install-boot-activate

[scenario_5_2]
log_marker      = Wait for node 3 activation completion
script          = [waitforlog]
fds_node        = node3
service         = sm
logentry        = :NotifyDLTCloseCb
occurrences     = 1
maxwait         = 240


[scenario_5_3]
log_marker      = Verify nodes node1, 2, and 3 are up
script          = [verify]
state           = up
# Comma separated list of nodes.
fds_nodes       = node1,node2,node3

[scenario_boot_activate_node_4]
log_marker      = Boot and activate a fourth node.
script          = [node4]
action          = install-boot-activate

[scenario_wait_sm4_dlt_close]
log_marker      = Wait for node 4 activation completion
script          = [waitforlog]
fds_node        = node4
service         = sm
logentry        = :NotifyDLTCloseCb
occurrences     = 1
maxwait         = 240

[scenario_verify_4nodes_up]
log_marker      = Verify nodes node1, 2, 3, and 4 are up
script          = [verify]
state           = up
# Comma separated list of nodes. No white space, please.
fds_nodes       = node1,node2,node3,node4

[scenario_wait_for_4SMs_deployed]
log_marker      = Wait for OM deploy DLT with 4 nodes
script          = [waitforlog]
fds_node        = node1
service         = om
logentry        = OM deployed DLT with 4 nodes
occurrences     = 1
maxwait         = 180

[scenario_wait_for_4DMs_deployed]
log_marker      = Wait for OM deploy DMT with 4 nodes
script          = [waitforlog]
fds_node        = node1
service         = om
logentry        = OM deployed DMT with 4 DMs
occurrences     = 1
maxwait         = 180

[scenario_wait_for_active_ios]
log_marker      = wait for active IOs
script          = [join]
join_scenario   = scenario_4_4_start_writes

# Run the SM Checkers to confirm migration.
[scenario_check_sm_migration]
# Run SM checker and compare results.
log_marker      = Check SM Migration from all SMs.
# Try this one even if previous scenarios have failed just to see what happens.
script          = [testcases.SmChk.TestVerifyMigrations]

[scenario_install_boot_activate_5th_node]
log_marker      = Boot and activate a fifth node.
script          = [node5]
action          = install-boot-activate

# wait for OM to start deploying SM 5
[scenario_wait_for_sm5_start_first_deploy]
log_marker      = Wait for SM5 start-DLT deployment
script          = [waitforlog]
fds_node        = node1
service         = om
logentry        = Added SMs: 1 Removed SMs: 0 Non-failed SMs: 5 Total SMs: 5
occurrences     = 1
maxwait         = 240

[scenario_shutdown_domain_before_migration_check]
log_marker      = Kill domain
script          = [domain]
action          = kill

# Now bringup whole domain including node5
# Since we shutdown the domain just after OM started deploying node5
# DLT most likely does not contain SM5 yet. If not, OM should start re-deploying SM5
[scenario_bringup_domain_after_stop_ungraceful]
log_marker      = Boot the domain without cleaning (no "install" option)
script          = [domain]
action          = boot

# we should see all 5 SMs in the domain
[scenario_wait_for_all_sm_ungraceful]
log_marker      = Wait for completion-DLT deployment
script          = [waitforlog]
fds_node        = node1
service         = om
logentry        = OM deployed DLT with 5 nodes
occurrences     = 1
maxwait         = 360
atleastone      = 1

[scenario_kill_dm_node1]
log_marker      = Shutdown the DMs on node1
script          = [service]
fds_node        = node1
service         = dm
action          = remove-verifydown
delay_wait      = 3

[scenario_kill_dm_node2]
log_marker      = Shutdown the DMs on node2
script          = [service]
fds_node        = node2
service         = dm
action          = remove-verifydown
delay_wait      = 3

[scenario_kill_dm_node3]
log_marker      = Shutdown the DMs on node3
script          = [service]
fds_node        = node3
service         = dm
action          = remove-verifydown
delay_wait      = 3

[scenario_kill_dm_node4]
log_marker      = Shutdown the DMs on node4
script          = [service]
fds_node        = node4
service         = dm
action          = remove-verifydown
delay_wait      = 3

[scenario_dmchecker]
log_marker      = run dm checker
script          = [testcases.TestFDSSysVerify.TestDMChecker]

[scenario_reenable_restart_failed_children_processes_node1]
log_marker      = Change platform.conf to restart killed processes automatically
script          = [testcases.TestFDSEnvMgt.TestModifyPlatformConf]
param_names     = current_string,replace_string,node
params          = restart_failed_children_processes.*, restart_failed_children_processes = true,node1

[scenario_reenable_restart_failed_children_processes_nodeall]
log_marker      = Change platform.conf to restart killed processes automatically
script          = [testcases.TestFDSEnvMgt.TestModifyPlatformConf]
param_names     = current_string,replace_string,node,applyAll
params          = restart_failed_children_processes.*, restart_failed_children_processes = true,node1,1

# Now clean up installation areas.
[scenario_shutdown_and_clean_domain]
log_marker      = Shutdown and cleanup.
script          = [domain]
action          = kill-uninst
